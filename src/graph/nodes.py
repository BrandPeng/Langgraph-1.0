"""
èŠ‚ç‚¹å®šä¹‰ - æ¯ä¸ª Agent çš„å…·ä½“å®ç°

æ¯ä¸ªèŠ‚ç‚¹å°±æ˜¯ä¸€ä¸ªå‡½æ•°ï¼š
- è¾“å…¥: State (å½“å‰çŠ¶æ€)
- è¾“å‡º: dict (è¦æ›´æ–°çš„çŠ¶æ€å­—æ®µ)
"""

import os
from pathlib import Path

from langchain.chat_models import init_chat_model
from langchain_openai import ChatOpenAI

from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage
from .state import State
from ..tools.tools import get_research_tools


# ============================================================
# è¾…åŠ©å‡½æ•°
# ============================================================

## è¯»å–æ¨¡å‹api é€‰æ‹©è‡ªå·±çš„æ¨¡å‹ è¿™é‡Œç”¨çš„æ˜¯ç™¾ç‚¼å¹³å°æä¾›çš„å¤§æ¨¡å‹api
ALIBABA_BASE_URL = os.environ.get("ALIBABA_BASE_URL")
ALIBABA_API_KEY = os.getenv("ALIBABA_API_KEY")
model = init_chat_model(
    "deepseek-v3",  # 1. æ¨¡å‹åç§°ï¼šè¯·åœ¨ç™¾ç‚¼æ§åˆ¶å°ç¡®è®¤å‡†ç¡®çš„ model ID
    model_provider="openai",  # 2. æä¾›å•†ï¼šå¿…é¡»å¡« openai
    base_url= ALIBABA_BASE_URL,  # 3. åœ°å€ï¼šç™¾ç‚¼çš„å…¼å®¹å…¥å£
    api_key=ALIBABA_API_KEY,  # 4. ä½ çš„ç™¾ç‚¼ API Key
)


def load_prompt(name: str) -> str:
    """åŠ è½½æç¤ºè¯æ–‡ä»¶"""
    prompt_path = Path(__file__).parent.parent / "prompts" / f"{name}.md"
    if prompt_path.exists():
        return prompt_path.read_text(encoding="utf-8")
    return ""


# ============================================================
# èŠ‚ç‚¹å®ç°
# ============================================================

def planner_node(state: State) -> dict:
    """
    è§„åˆ’å™¨èŠ‚ç‚¹ - åˆ¶å®šç ”ç©¶è®¡åˆ’
    
    è¾“å…¥: ç”¨æˆ·çš„ä»»åŠ¡
    è¾“å‡º: æ‰§è¡Œè®¡åˆ’
    """
    print("\nğŸ¯ [è§„åˆ’å™¨] æ­£åœ¨åˆ¶å®šè®¡åˆ’...")
    
    llm = model
    system_prompt = load_prompt("planner")
    
    # æ„å»ºæ¶ˆæ¯
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"è¯·ä¸ºä»¥ä¸‹ä»»åŠ¡åˆ¶å®šç ”ç©¶è®¡åˆ’ï¼š\n\n{state['task']}")
    ]
    
    # è°ƒç”¨ LLM
    response = llm.invoke(messages)
    plan = response.content
    
    print(f"ğŸ“‹ è®¡åˆ’å·²ç”Ÿæˆ:\n{plan}\n")
    
    # è¿”å›è¦æ›´æ–°çš„çŠ¶æ€
    return {
        "plan": plan,
        "messages": [response]
    }


def researcher_node(state: State) -> dict:
    """
    ç ”ç©¶å‘˜èŠ‚ç‚¹ - æœç´¢å’Œæ”¶é›†ä¿¡æ¯
    
    è¾“å…¥: æ‰§è¡Œè®¡åˆ’
    è¾“å‡º: ç ”ç©¶ç»“æœ
    """
    print("\nğŸ” [ç ”ç©¶å‘˜] æ­£åœ¨æ”¶é›†ä¿¡æ¯...")
    
    llm = model
    system_prompt = load_prompt("researcher")
    
    # ç»‘å®šå·¥å…·åˆ° LLM
    tools = get_research_tools()
    llm_with_tools = llm.bind_tools(tools)
    
    # æ„å»ºæ¶ˆæ¯
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"""
ä»»åŠ¡: {state['task']}

ç ”ç©¶è®¡åˆ’:
{state['plan']}

è¯·æ ¹æ®è®¡åˆ’æœç´¢ç›¸å…³ä¿¡æ¯ã€‚
""")
    ]
    
    # ç¬¬ä¸€æ¬¡è°ƒç”¨ - LLM å†³å®šæ˜¯å¦ä½¿ç”¨å·¥å…·
    response = llm_with_tools.invoke(messages)

    # å¦‚æœ LLM æƒ³è¦è°ƒç”¨å·¥å…·
    if response.tool_calls:
        print(f"ğŸ”§ è°ƒç”¨å·¥å…·: {[tc['name'] for tc in response.tool_calls]}")

        # å…ˆæŠŠ assistant çš„å“åº”åŠ å…¥æ¶ˆæ¯åˆ—è¡¨
        messages.append(response)

        # æ‰§è¡Œæ¯ä¸ªå·¥å…·è°ƒç”¨ï¼Œå¹¶ç”¨ ToolMessage è¿”å›ç»“æœ
        for tool_call in response.tool_calls:
            tool_name = tool_call["name"]
            tool_args = tool_call["args"]
            tool_call_id = tool_call["id"]  # è·å– tool_call_id

            # æ‰¾åˆ°å¹¶æ‰§è¡Œå¯¹åº”çš„å·¥å…·
            result = "å·¥å…·æœªæ‰¾åˆ°"
            for tool in tools:
                if tool.name == tool_name:
                    result = tool.invoke(tool_args)
                    break

            # ä½¿ç”¨ ToolMessage è¿”å›ç»“æœï¼ˆå¿…é¡»æŒ‡å®š tool_call_idï¼‰
            messages.append(ToolMessage(
                content=str(result),
                tool_call_id=tool_call_id
            ))

        # è®© LLM æ•´ç†å·¥å…·è¿”å›çš„ç»“æœ
        response = llm.invoke(messages)
    
    research_results = response.content
    print(f"ğŸ“š ç ”ç©¶å®Œæˆï¼Œæ”¶é›†åˆ°ä¿¡æ¯\n")
    
    return {
        "research_results": research_results,
        "messages": [response]
    }


def writer_node(state: State) -> dict:
    """
    å†™ä½œè€…èŠ‚ç‚¹ - ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    
    è¾“å…¥: ç ”ç©¶ç»“æœ
    è¾“å‡º: æœ€ç»ˆç­”æ¡ˆ
    """
    print("\nâœï¸ [å†™ä½œè€…] æ­£åœ¨æ’°å†™ç­”æ¡ˆ...")
    
    llm = model
    system_prompt = load_prompt("writer")
    
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"""
ç”¨æˆ·é—®é¢˜: {state['task']}

ç ”ç©¶ç»“æœ:
{state['research_results']}

è¯·æ ¹æ®ä»¥ä¸Šç ”ç©¶ç»“æœï¼Œæ’°å†™ä¸€ä»½å®Œæ•´çš„å›ç­”ã€‚
""")
    ]
    
    response = llm.invoke(messages)
    final_answer = response.content
    
    print(f"âœ… ç­”æ¡ˆå·²ç”Ÿæˆ\n")
    
    return {
        "final_answer": final_answer,
        "messages": [response]
    }
